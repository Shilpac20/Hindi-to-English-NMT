{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-LSTM_with_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh5SOoJONg5c"
      },
      "source": [
        "**Load Data and do preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AudXXgJrhZnm"
      },
      "source": [
        "***I have mounted my drive and loaded our training dataset in a list and in the csv file i have only used columns hindi and english and left the index column that was present in the csv.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z0UMC1bFFbS",
        "outputId": "5b709f05-6261-4bf1-e0c4-591339032538"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by3hgALVNdEb"
      },
      "source": [
        "import csv\n",
        "column_names = []\n",
        "dataset = []\n",
        "source_sent_list=[]\n",
        "targ_sent_list=[]\n",
        "  \n",
        "# reading csv file\n",
        "with open('/content/drive/MyDrive/train.csv', 'r') as csvfile:\n",
        "    # creating a csv reader object\n",
        "    csvreader = csv.reader(csvfile)\n",
        "      \n",
        "    # extracting field names through first row\n",
        "    column_names = next(csvreader)\n",
        "  \n",
        "    # extracting each data row one by one\n",
        "    for row in csvreader:\n",
        "        #print(row[1],row[2])\n",
        "        dataset.append([row[1],row[2]]) #row[1] contains hindi sentences and row[2] contains english sentences\n",
        "        source_sent_list.append(row[1])\n",
        "        targ_sent_list.append(row[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgEWVg0vVmU"
      },
      "source": [
        "**Brief Overview of the pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZk_CjVvfQN"
      },
      "source": [
        "*Unprocessed text ---> Pre-processing and tokenizing ----> Building the vocabulary for source(Hindi) and target(english)  using the tokenized text --->Converting each sentence to tensor objects using the indexes(in the vocabulary) of the words present in the sentence ---> passing it to the encoder decoder model and training it .*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5lL19WZOcbi"
      },
      "source": [
        "**Pre-requisite for hindi text processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0aLSEpLsZOV"
      },
      "source": [
        "***Cloning the indic_nlp_library and setting the path . indic_nlp library will be used pre-processing the hindi text provided in training dataset.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzBCqDtYOYh4",
        "outputId": "a823a0b7-79c6-4a68-fac4-40da95cd00e1"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1271, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1271 (delta 50), reused 54 (delta 25), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1271/1271), 9.56 MiB | 13.35 MiB/s, done.\n",
            "Resolving deltas: 100% (654/654), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 133, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 133 (delta 0), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (133/133), 149.77 MiB | 22.37 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Collecting Morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKqDE1NrtUaT"
      },
      "source": [
        "***Importing libraries for pre-processing Hindi and English Text***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2xQ3YUZP1uG"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import random\n",
        "from unicodedata import normalize\n",
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nwg1oQFvE6T"
      },
      "source": [
        "***Here I am building my vocabulary where SOS_token is the start token , EOS_token is the end token , UNK_token is for unknown token, PAD_token is for padding.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngG57vckOxWd"
      },
      "source": [
        "SOS_token = '<start>' #start token\n",
        "EOS_token = '<end>'   #end token\n",
        "UNK_token = '<unk>'   #unknown token\n",
        "PAD_token = '<pad>'   #pad token\n",
        "\n",
        "SOS_idx = 0  #index for start token\n",
        "EOS_idx = 1  #index for end token\n",
        "UNK_idx = 2  #index for unknown token\n",
        "PAD_idx = 3  #index for pad token\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        # name denotes the name of the language sent as input i.e. Hindi or English\n",
        "        self.name = name  \n",
        "\n",
        "        #word2count is used to count the times a word appreas in the vocabulary\n",
        "        self.word2count = {}\n",
        "\n",
        "        # n_words denotes the number of unique words in constructed vocabulary\n",
        "        self.n_words = 4  # Count SOS,EOS,PAD,UNK\n",
        "\n",
        "        #index2word provides the word from the vocabulary we will build given  the index\n",
        "        self.index2word = {\n",
        "            SOS_idx: SOS_token,\n",
        "            EOS_idx: EOS_token,\n",
        "            UNK_idx: UNK_token,\n",
        "            PAD_idx: PAD_token\n",
        "        }\n",
        "\n",
        "        #word2index provides the index from the vocabulary we will build given  the word\n",
        "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "   \n",
        "    #given a sentence I am splitting it in words and adding it to our vocabulary\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            # since my index starts from 0 and I have initialised n_words to 4 and intitially my vocab contains just SOS,EOS,PAD,UNK , I am putting the n_words count as index of the current word encountered\n",
        "            self.word2index[word] = self.n_words \n",
        "             \n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n3yWm-i2jkV"
      },
      "source": [
        "***Here I have defined functions for cleaning the source(hindi) text and target(english) text***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anqq7JCDO-Ui"
      },
      "source": [
        "#clean_text function takes in the text and it removes puntuation marks, numbers,quotes from the provided text and returns the cleaned text\n",
        "def clean_text(text):\n",
        "    text = text.replace(u',','')\n",
        "    text = text.replace(u'\"','')\n",
        "    text = text.replace(u'\"','')\n",
        "    text = text.replace(u\"‘‘\",'')\n",
        "    text = text.replace(u\"’’\",'')\n",
        "    text = text.replace(u\"''\",'')\n",
        "    text = text.replace(u\"।\",'')\n",
        "    text=text.replace(u',','')\n",
        "    text=text.replace(u'\"','')\n",
        "    text=text.replace(u'(','')\n",
        "    text=text.replace(u')','')\n",
        "    text=text.replace(u'\"','')\n",
        "    text=text.replace(u':','')\n",
        "    text=text.replace(u\"'\",'')\n",
        "    text=text.replace(u\"‘‘\",'')\n",
        "    text=text.replace(u\"’’\",'')\n",
        "    text=text.replace(u\"''\",'')\n",
        "    text=text.replace(u\".\",'')\n",
        "    text=text.replace(u\"-\",'')\n",
        "    text=text.replace(u\"।\",'')\n",
        "    text=text.replace(u\"?\",'')\n",
        "    text=text.replace(u\"\\\\\",'')\n",
        "    text=text.replace(u\"_\",'')\n",
        "    text=text.replace(\"'\", \"\")\n",
        "    text=text.replace('\"', \"\")\n",
        "    text= re.sub(\"'\", '', text)\n",
        "    text= re.sub(\"’\", '', text)\n",
        "    text=re.sub('[0-9+\\-*/.%]', '', text)\n",
        "    text=text.strip()\n",
        "    text=re.sub(' +', ' ',text)\n",
        "    exclude = set(string.punctuation)\n",
        "    text= ''.join(ch for ch in text if ch not in exclude)\n",
        "    return text\n",
        "\n",
        "#this function is used to pre-process english sentences\n",
        "def pre_process_english_sentence(line):\n",
        "    #lower casing the text\n",
        "    line = line.lower()\n",
        "    #cleaning the text \n",
        "    line = clean_text(line)\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    #I first normalise the sentences using unicode normalise to NDF(cannonical decomposition) and then convert to ascii ignoring the errors\n",
        "    line = normalize('NFD', line).encode('ascii', 'ignore') \n",
        "    line = line.decode('UTF-8')\n",
        "    line = line.split()\n",
        "    line = [re_print.sub('', w) for w in line]\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "    line = ' '.join(line)\n",
        "    return line\n",
        "\n",
        "#this function is used to pre-process hindi sentences\n",
        "def pre_process_hindi_sentence(line):\n",
        "    #cleaning the text\n",
        "    line = clean_text(line)\n",
        "    factory = IndicNormalizerFactory()\n",
        "    #remove_nuktas is set to false as it usually removes things like vishargya from the text and I did not need that to be done\n",
        "    normalizer = factory.get_normalizer(\"hi\",remove_nuktas=False)\n",
        "    line = normalizer.normalize(line)\n",
        "    tokens = list()\n",
        "    #tokenising the text using trial_tokenize of indic library\n",
        "    for t in indic_tokenize.trivial_tokenize(line):\n",
        "        tokens.append(t)\n",
        "    line = tokens\n",
        "    line = [word for word in line if not re.search(r'\\d', word)]\n",
        "    line = ' '.join(line)\n",
        "    return (line)\n",
        "\n",
        "#this functions appends the preprocessed data to a list of list \n",
        "def to_pairs(source_sent_list,targ_sent_list):\n",
        "    english_lines =targ_sent_list\n",
        "    hindi_lines = source_sent_list\n",
        "    pairs = []\n",
        "    for i in range(len(hindi_lines)):\n",
        "        pairs.append([])\n",
        "        pairs[i].append(pre_process_hindi_sentence(hindi_lines[i]))\n",
        "        pairs[i].append(pre_process_english_sentence(english_lines[i]))\n",
        "    return pairs\n",
        "\n",
        "#this preparesmy vocabulary for hindi and english language by using the words in the training dataset\n",
        "def prepareData(pairs):\n",
        "    input_lang = Lang('hin')\n",
        "    output_lang = Lang('eng')\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    return input_lang, output_lang\n",
        "\n",
        "\n",
        "pairs_all = to_pairs(source_sent_list,targ_sent_list) #pairs_all is a list of list containing processed hindi and its corresponding english text\n",
        "input_lang, output_lang = prepareData(pairs_all) #input_lang and output_lang are objects of the Lang class defined earlier which contains information about the vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpzR8_gSOyD5"
      },
      "source": [
        "***Let us check if the pairs_all has been constructed correctly***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfbVSngRhxlQ",
        "outputId": "0e16ee36-552e-4ba6-a2f4-1e8e492137fe"
      },
      "source": [
        "for i,j in enumerate(pairs_all):\n",
        "  print(i,j)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 ['एल सालवाडोर मे जिन दोनो पक्षों ने सिविलयुद्ध से वापसी ली उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं', 'in el salvador both sides that withdrew from their civil war took moves that had been proven to mirror a prisoners dilemma strategy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITJcwpO6Pr1U"
      },
      "source": [
        "***Let us now split the pairs all into two lists one containing all source sentences and the other containing the target sentences . This will be helpful while constructing the train and the validation dataset.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-N_j488hTZq"
      },
      "source": [
        "source_to_target = {}\n",
        "i=0\n",
        "for index,source_target in enumerate(pairs_all):\n",
        "    source=source_target[0]\n",
        "    target=source_target[1]\n",
        "    if source in source_to_target:\n",
        "        source_to_target[source].append(target)\n",
        "    else:\n",
        "        source_to_target[source] = [target]\n",
        "source_sents, target_sents = zip(*source_to_target.items())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xMnaTcyQGFH"
      },
      "source": [
        "***Checking if my source_sents are correctly added***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msw5waoypYYh",
        "outputId": "89ab2fad-699f-48ef-ea27-0b2639211e9c"
      },
      "source": [
        "for i in target_sents:\n",
        "  print(i)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['in el salvador both sides that withdrew from their civil war took moves that had been proven to mirror a prisoners dilemma strategy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Hl1P4Qi8j_"
      },
      "source": [
        "**Splitting into train,val data (95%,5%) as test set was provided every week**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv4SSbCTjD8D"
      },
      "source": [
        "import numpy as np\n",
        "#used random_seed to ensure that my every split is uniform\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "source_length = len(source_sents)\n",
        "inidices = np.random.permutation(source_length)\n",
        "\n",
        "#spliting the dataset in the ratio of 9:1  (train:validation ratio)\n",
        "training_indices = inidices[:int(source_length*0.95)] \n",
        "dev_indices = inidices[int(source_length*0.95):]\n",
        "\n",
        "#preparing the training dataset\n",
        "training_source = [source_sents[i] for i in training_indices]\n",
        "dev_source = [source_sents[i] for i in dev_indices]\n",
        "\n",
        "training_target = [target_sents[i] for i in training_indices]\n",
        "dev_target = [target_sents[i] for i in dev_indices]\n",
        "\n",
        "# Unwrap training examples\n",
        "training_t = []\n",
        "training_s = []\n",
        "for source, tt in zip(training_source, training_target):\n",
        "    for target in tt:\n",
        "        training_t.append(target)\n",
        "        training_s.append(source)\n",
        "\n",
        "training_source = training_s\n",
        "training_target = training_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyGED2ybRigC"
      },
      "source": [
        "***Checking the number of words in my hindi vocabulary (constructed from the training dataset)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLAceR3vRdFp",
        "outputId": "23537f7a-7a83-4e7d-ad00-86a33fa0d70c"
      },
      "source": [
        "input_lang.n_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46185"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCCqfzN6RsMH"
      },
      "source": [
        "***Checking the number of words in my english vocabulary(constructed from training dataset)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFm3VLRnRkPH",
        "outputId": "c6ae1ed2-bf19-4022-c44c-8eafb4afff61"
      },
      "source": [
        "output_lang.n_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL9cf891Sg3o"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ko4tbIsfA7"
      },
      "source": [
        "***checking the maximum length of the source and target sentence so that it is helpful while padding the shorter sentences , to make all of same length and then convert to a tensor object.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sptIswrUSSkl",
        "outputId": "4bffdecf-d168-473e-890f-aeaf16ebd68c"
      },
      "source": [
        "lenght_list=[]\n",
        "for l in source_sents:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_src = np.max(lenght_list)\n",
        "print (max_length_src)\n",
        "lenght_list=[]\n",
        "for l in target_sents:\n",
        "    lenght_list.append(len(str(l).split(' ')))\n",
        "max_length_tar = np.max(lenght_list)\n",
        "print (max_length_tar)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "394\n",
            "301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBlRZWKTbxr"
      },
      "source": [
        "**Starting the Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAcCmwKptR0k"
      },
      "source": [
        "***Importing the necessary libraries for our model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adp2Ms1lUcB7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "#setting the device to cuda if its available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnSWEfaOTeu1"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCrNUiKjTk2Z"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        init.normal_(self.embedding.weight, 0.0, 0.2)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            hidden_size,\n",
        "            int(hidden_size/2),  # Bi-directional processing will ouput vectors of double size, therefore I reduced output dimensionality\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,  # First dimension of input tensor will be treated as a batch dimension\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "    # word_inputs: (batch_size, seq_length), h: (h_or_c, layer_n_direction, batch, seq_length)\n",
        "    def forward(self, word_inputs, hidden):         \n",
        "        # embedded (batch_size, seq_length, hidden_size)\n",
        "        embedded = self.embedding(word_inputs)\n",
        "        # output (batch_size, seq_length, hidden_size*directions)\n",
        "        # hidden (h: (num_layers*directions, batch_size, hidden_size),\n",
        "        #         c: (num_layers*directions, batch_size, hidden_size))\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batches):\n",
        "        h_s = torch.zeros(self.n_layers*2, batches, int(self.hidden_size/2)).contiguous().to(device)\n",
        "        c_s = torch.zeros(self.n_layers*2, batches, int(self.hidden_size/2)).contiguous().to(device)\n",
        "        hidden = (h_s, c_s)\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbYxkTozui47"
      },
      "source": [
        "***I have just tested my encoded with a small dummy example to make sure it is working properly***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoE7z1V-9H3w",
        "outputId": "c615870c-b3f0-488b-eab3-8298aea25c50"
      },
      "source": [
        "vocab_size = 10\n",
        "hidden_dim = 10\n",
        "n_layers = 1\n",
        "\n",
        "encoder_test = EncoderRNN(vocab_size, hidden_dim, n_layers).to(device)\n",
        "print(encoder_test)\n",
        "\n",
        "# Recurrent network requires initial hidden state\n",
        "encoder_hidden = encoder_test.init_hidden(1)\n",
        "\n",
        "# Test input of size (1x3), one sequence of size 3\n",
        "word_input = torch.LongTensor([[1, 2, 3]]).to(device)\n",
        "#print(word_input.shape,\" \",encoder_hidden.shape)\n",
        "\n",
        "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
        "\n",
        "# encoder_outputs: (batch_size, seq_length, hidden_size)\n",
        "# encoder_hidden[0, 1]: (n_layers*2, batch_size, hidden_size/2)\n",
        "print(encoder_outputs.shape, encoder_hidden[0].shape, encoder_hidden[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderRNN(\n",
            "  (embedding): Embedding(10, 10)\n",
            "  (lstm): LSTM(10, 5, batch_first=True, bidirectional=True)\n",
            ")\n",
            "torch.Size([1, 3, 10]) torch.Size([2, 1, 5]) torch.Size([2, 1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6hrBKPCUf-D"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roHva2vkDkRW"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        #For Attention \n",
        "        self.weight1=nn.Linear(hidden_size,hidden_size)\n",
        "        self.weight2=nn.Linear(hidden_size,hidden_size)\n",
        "        self.v=nn.Linear(hidden_size,1)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            hidden_size+hidden_size,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,  # First dimension of input tensor will be treated as a batch dimension\n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.output=nn.Linear(hidden_size,vocab_size)\n",
        "        self.softmax=nn.LogSoftmax(dim=2)\n",
        "  def forward(self, word_inputs, hidden,encoder_outputs):\n",
        "        # we run this one by one\n",
        "        # embedded (batch_size, 1, hidden_size)\n",
        "        embedded = self.embedding(word_inputs.to(device)).unsqueeze_(1)\n",
        "        temp_hidden=hidden[0].permute(1,0,2)\n",
        "        #print(self.weight1(temp_hidden).squeeze(1).size(),\" \",self.weight2(encoder_outputs).squeeze(1).size())\n",
        "        attn_score=torch.tanh(self.weight1(temp_hidden) + self.weight2(encoder_outputs))\n",
        "        attn_weights=F.softmax(self.v(attn_score),dim=1)\n",
        "        context_vec=torch.sum(attn_weights * encoder_outputs,dim=1,keepdim=True)\n",
        "        #print(attn_score.size(),\" \",attn_weights.size(),\" \",encoder_outputs.size(),\" \",context_vec.size(),\" \",embedded.size())\n",
        "        #embedded_temp=embedded.permute(1, 0, 2).reshape(1, 2, 10).permute(1, 0, 2)\n",
        "        output=torch.cat((embedded,context_vec),-1)\n",
        "        #print(output.size(),hidden[0].size())\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVBT5sB42z7W"
      },
      "source": [
        "***I did a small test for the decoder code as well to make sure it works properly.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTy5o51P-8LE",
        "outputId": "8f6751f5-5df8-49b9-fbc9-aa9865061025"
      },
      "source": [
        "decoder_test = DecoderRNN(vocab_size, hidden_dim, n_layers).to(device)\n",
        "print(decoder_test)\n",
        "\n",
        "word_inputs = torch.LongTensor([[1, 2, 3]]).to(device)\n",
        "\n",
        "decoder_hidden_h = encoder_hidden[0].permute(1, 0, 2).reshape(1, 1, 10).permute(1, 0, 2)\n",
        "decoder_hidden_c = encoder_hidden[1].permute(1, 0, 2).reshape(1, 1, 10).permute(1, 0, 2)\n",
        "#print(decoder_hidden_h.size())\n",
        "\n",
        "for i in range(3):\n",
        "    input = word_inputs[:, i]\n",
        "    decoder_output, decoder_hidden = decoder_test(input, (decoder_hidden_h, decoder_hidden_c),encoder_outputs)\n",
        "    decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
        "    print(decoder_output.size(), decoder_hidden_h.size(), decoder_hidden_c.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderRNN(\n",
            "  (embedding): Embedding(10, 10)\n",
            "  (weight1): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (weight2): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (v): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (lstm): LSTM(20, 10, batch_first=True)\n",
            "  (output): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (softmax): LogSoftmax(dim=2)\n",
            ")\n",
            "torch.Size([1, 1, 10]) torch.Size([1, 1, 10]) torch.Size([1, 1, 10])\n",
            "torch.Size([1, 1, 10]) torch.Size([1, 1, 10]) torch.Size([1, 1, 10])\n",
            "torch.Size([1, 1, 10]) torch.Size([1, 1, 10]) torch.Size([1, 1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRJy0ZBnx_Tm"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-WMjRwwMyWc"
      },
      "source": [
        "***Putting the encoder decoder structure to build the seq2seq model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m15adye3wCuf"
      },
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, n_layers,device):\n",
        "        super(Seq2seq, self).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        #EncoderRNN takes in the input(hindi) language vocabulary size , hidden dimension and also the number of layers\n",
        "        #I did to(self.device) to ensure the input and the hidden layer outputs are in the same device while running on GPUs\n",
        "        self.encoder = EncoderRNN(input_vocab_size, hidden_size, self.n_layers).to(self.device)\n",
        "\n",
        "        #DecoderRNN takes in the output(english) language vocabulary size , hidden dimension and also the number of layers\n",
        "        #I did to(self.device) to ensure the input and the hidden layer outputs are in the same device while running on GPUs\n",
        "        self.decoder = DecoderRNN(output_vocab_size, hidden_size, self.n_layers).to(self.device)\n",
        "\n",
        "        self.W = nn.Linear(hidden_size, output_vocab_size)\n",
        "        init.normal_(self.W.weight, 0.0, 0.2) #initialise the weights\n",
        "\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "    def _forward_encoder(self, x):\n",
        "        # x is (batch size, sequence length)\n",
        "        batch_size = x.shape[0]\n",
        "        #initialising the hidden inputs according to the batch_size\n",
        "        init_hidden = self.encoder.init_hidden(batch_size)\n",
        "        #send the input x and use .to(device) on x to ensure it is in the same device as init_hidden while using GPUs\n",
        "        encoder_outputs, encoder_hidden = self.encoder(x.to(device), init_hidden)\n",
        "        encoder_hidden_h, encoder_hidden_c = encoder_hidden\n",
        "        self.encoder_outputs=encoder_outputs\n",
        "        self.decoder_hidden_h = encoder_hidden_h.contiguous().permute(1,0,2).reshape(batch_size, self.n_layers, self.hidden_size).permute(1,0,2)\n",
        "        self.decoder_hidden_c = encoder_hidden_c.contiguous().permute(1,0,2).reshape(batch_size, self.n_layers, self.hidden_size).permute(1,0,2)\n",
        "        return self.decoder_hidden_h, self.decoder_hidden_c,self.encoder_outputs\n",
        "\n",
        "    def forward_train(self, x, y):\n",
        "        decoder_hidden_h, decoder_hidden_c ,encoder_outputs= self._forward_encoder(x)\n",
        "        #print(decoder_hidden_h.is_contiguous,\" \",decoder_hidden_c.is_contiguous)\n",
        "        H = []\n",
        "        for i in range(y.shape[1]):\n",
        "            input = y[:, i]\n",
        "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h.contiguous(), decoder_hidden_c.contiguous()),encoder_outputs)\n",
        "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
        "            # h: (batch_size, vocab_size)\n",
        "            h = self.W(decoder_output.squeeze(1))\n",
        "            # h: (batch_size, vocab_size, 1)\n",
        "            H.append(h.unsqueeze(2))\n",
        "\n",
        "        # H: (batch_size, vocab_size, seq_len)\n",
        "        return torch.cat(H, dim=2)\n",
        "\n",
        "    #this is used during test time\n",
        "    def forward(self, x):\n",
        "        decoder_hidden_h, decoder_hidden_c,encoder_outputs = self._forward_encoder(x)\n",
        "\n",
        "        current_y = SOS_idx\n",
        "        result = [current_y]\n",
        "        counter = 0\n",
        "        #I have set the limit of counter to be 100 as i did not want my predicted sentences to be too long\n",
        "        while current_y != EOS_idx and counter < 100:\n",
        "            input = torch.tensor([current_y])\n",
        "            decoder_output, decoder_hidden = self.decoder(input, (decoder_hidden_h, decoder_hidden_c),encoder_outputs)\n",
        "            decoder_hidden_h, decoder_hidden_c = decoder_hidden\n",
        "            # h: (vocab_size)\n",
        "            h = self.W(decoder_output.squeeze(1)).squeeze(0)\n",
        "            y = self.softmax(h)\n",
        "            _, current_y = torch.max(y, dim=0)\n",
        "            current_y = current_y.item()\n",
        "            result.append(current_y)\n",
        "            counter += 1\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT5AUEg5U_zn"
      },
      "source": [
        "**We have prepared input output pairs which are strings but input to our model should be tensors. So need to convert to tensors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-gabHwvlijN"
      },
      "source": [
        "import os \n",
        "data_dir='/content/drive/MyDrive/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwR3XSiDk5JN"
      },
      "source": [
        "#this function takes in the lang object(which denotes the either hindi vocab set or english vocab set) and the sentence and converts each word to its index\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "#this function takes the lang object(which denotes the either hindi vocab set or english vocab set) and the sentence and also the type(which is either hindi or english)\n",
        "def tensorFromSentence(lang, sentence,type):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_idx) #appended the end of sentence token at the end of the sentence\n",
        "    indexes.insert(0, SOS_idx) #appended the start of sentence token at the start of the sentence\n",
        "    if type=='hindi':\n",
        "      max_seq_length=2+max_length_src #if the type is hindi the max length is set to 2+the max length of hindi sentences encountered in the training dataset(2 because we appended 2 tokens)\n",
        "    else:\n",
        "      max_seq_length=2+max_length_tar  #if the type is english the max length is set to 2+the max length of english sentences encountered in the training dataset(2 because we appended 2 tokens)\n",
        "    if len(indexes) < max_seq_length:\n",
        "        indexes += [PAD_idx] * (max_seq_length - len(indexes)) # padding shorter sentences with the pad token index\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device) # converting the indexes to a tensor object\n",
        "\n",
        "\n",
        "def tensorsFromPair(source,target):\n",
        "    #I unsqeezed the tensors as I wanted the size to be (sequence length) and not (1,sequence length)\n",
        "    input_tensor = tensorFromSentence(input_lang, source,'hindi').unsqueeze(1)\n",
        "    target_tensor = tensorFromSentence(output_lang, target,'english').unsqueeze(1)\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "#constructing training and validation set tensors for source(hindi) and target(english) language\n",
        "training = []\n",
        "for source_sent, target_sent in zip(training_source, training_target):\n",
        "    #print(source_sent)\n",
        "    training.append(tensorsFromPair(source_sent, target_sent))\n",
        "\n",
        "x_training, y_training = zip(*training)\n",
        "x_training = torch.transpose(torch.cat(x_training, dim=-1), 1, 0)\n",
        "y_training = torch.transpose(torch.cat(y_training, dim=-1), 1, 0)\n",
        "\n",
        "\n",
        "x_development = []\n",
        "for source_sent in dev_source:\n",
        "    tensor = tensorFromSentence(input_lang, source_sent, 'hindi').unsqueeze(1)\n",
        "    x_development.append(tensor)\n",
        "\n",
        "x_development = torch.transpose(torch.cat(x_development, dim=-1), 1, 0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqbkdkW6kb8r"
      },
      "source": [
        "***The unindex_words function will be used during the test time when the decoder provides us the indices of the predicted words we translate the indices back to words***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2Q_8-d6JjR"
      },
      "source": [
        "def unindex_words(lang, indices):\n",
        "        return [lang.index2word[i] for i in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNZRZrqtVb32"
      },
      "source": [
        "**Training our model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzGIr9HLyW_x"
      },
      "source": [
        "from torch.optim import Adam\n",
        "#initialising our seq2seq model with no. of layers as 1 and hidden dimension as 256\n",
        "model = Seq2seq(input_lang.n_words, output_lang.n_words, 256, 1,device).to(device)\n",
        "#I have used Adam as my optimiser and have set my initial learning rate to 0.0001\n",
        "optim = Adam(model.parameters(), lr=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h47b8sYEylre",
        "outputId": "69cc7658-89db-454c-a9f1-7263889ee999"
      },
      "source": [
        "#checking specifications of my model\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): EncoderRNN(\n",
              "    (embedding): Embedding(46185, 256)\n",
              "    (lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (decoder): DecoderRNN(\n",
              "    (embedding): Embedding(32596, 256)\n",
              "    (weight1): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (weight2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (v): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (lstm): LSTM(512, 256, batch_first=True)\n",
              "    (output): Linear(in_features=256, out_features=32596, bias=True)\n",
              "    (softmax): LogSoftmax(dim=2)\n",
              "  )\n",
              "  (W): Linear(in_features=256, out_features=32596, bias=True)\n",
              "  (softmax): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlYJAORnytPz"
      },
      "source": [
        "**We will train our model in batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsR02U0gyx-h"
      },
      "source": [
        "import math\n",
        "#since I wanted to train my model in batches the batch_generator function takes in the size of the input to be batched and the batch size\n",
        "#the function yields the set of indices required for the current batch\n",
        "def batch_generator(batch_indices, batch_size):\n",
        "    batches = math.ceil(len(batch_indices)/batch_size)\n",
        "    for i in range(batches):\n",
        "        batch_start = i*batch_size\n",
        "        batch_end = (i+1)*batch_size\n",
        "        if batch_end > len(batch_indices):\n",
        "            yield batch_indices[batch_start:]\n",
        "        else:\n",
        "            yield batch_indices[batch_start:batch_end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weoWp8KDy5M4"
      },
      "source": [
        "# for calculating the loss I have used the CrossEntropyLoss function\n",
        "cross_entropy = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybdc4XP4MasF",
        "outputId": "0e5856ca-0b57-4c6d-8154-e83e00a3bad4"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9s5lsso18I"
      },
      "source": [
        "***I have used corpus_bleu as a scoring metric to check my models performance on validation set(constructed from the provided training dataset) across different epochs***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Giegt7CHy6MQ"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def bleu(n):\n",
        "    weights = [1.0/n]*n + [0.0]*(4-n)\n",
        "    return lambda list_of_references, list_of_hypothesis: corpus_bleu(list_of_references, list_of_hypothesis, weights)\n",
        "\n",
        "def accuracy(list_of_references, list_of_hypothesis):\n",
        "    total = 0.0\n",
        "    for references, hypothesis in zip(list_of_references, list_of_hypothesis):\n",
        "        total += 1.0 if tuple(hypothesis) in set(references) else 0.0\n",
        "    return total / len(list_of_references)\n",
        "\n",
        "score_functions = {'BLEU-{}'.format(i):bleu(i) for i in range(1, 5)}\n",
        "score_functions['Accuracy'] = accuracy\n",
        "\n",
        "def score(model, X, target, desc='Scoring...'):\n",
        "    scores = {name:0.0 for name in score_functions.keys()}\n",
        "    length = len(target)\n",
        "    list_of_hypothesis = []\n",
        "    '''\n",
        "    for i, x in tqdm(enumerate(X),\n",
        "                     desc=desc,\n",
        "                     total=length):\n",
        "    '''\n",
        "    for i, x in enumerate(X):\n",
        "        y = model(x.unsqueeze(0).to(device))\n",
        "        hypothesis = unindex_words(output_lang,y[1:-1])  # Remove SOS and EOS from y\n",
        "        list_of_hypothesis.append(hypothesis)\n",
        "    print(list_of_hypothesis)\n",
        "    print(target)\n",
        "    \n",
        "    for name, func in score_functions.items():\n",
        "        score = func(target, list_of_hypothesis)\n",
        "        scores[name] = score\n",
        "\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzz5m5m6p8Fy"
      },
      "source": [
        "***I have used tqdm_notebook just to monitor the progress***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47H_HdnkzAgi"
      },
      "source": [
        "#from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "BATCH_SIZE = 16 # i have taken batch size to be 16\n",
        "total_batches = int(len(x_training)/BATCH_SIZE) + 1  # total batches denote the numer of batches of size 16 \n",
        "indices = list(range(len(x_training))) # it takes the length of the training set\n",
        "\n",
        "early_stop_after = 3 # early stop counter is 3 so that we can do a early stop if we see no improvement for 3 back to back epochs\n",
        "early_stop_counter = 0\n",
        "best_model = None\n",
        "\n",
        "best_score = 0.0\n",
        "scoring_metric = 'BLEU-1'\n",
        "scores_history = []\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(1):\n",
        "    # Training\n",
        "    total_loss = 0.0\n",
        "    '''\n",
        "    this part to be used only when we have a model state to start with\n",
        "    \n",
        "     \n",
        "    if epoch==0:\n",
        "      model.load_state_dict(torch.load('/content/drive/MyDrive/NLP_Challenge_Test_Phase/model_best_lstm_attn_26_April.pt'))\n",
        "    '''\n",
        "   \n",
        "    for step, batch in enumerate(batch_generator(indices, BATCH_SIZE)):\n",
        "    \n",
        "    #for step, batch in enumerate(batch_generator(indices, BATCH_SIZE)):\n",
        "            x = x_training[batch, :].to(device)\n",
        "            # y for teacher forcing is all sequence without a last element\n",
        "            y_tf = y_training[batch, :-1].to(device)\n",
        "            # y for loss calculation is all sequence without a last element\n",
        "            y_true = y_training[batch, 1:].to(device)\n",
        "            # (batch_size, vocab_size, seq_length)\n",
        "            H = model.forward_train(x, y_tf)\n",
        "            loss = cross_entropy(H, y_true)\n",
        "\n",
        "            assert loss.item() > 0 # made sure that the loss is always >0 \n",
        "\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    loss_history.append(total_loss/total_batches)\n",
        "    torch.save(model.state_dict(), '/content/drive/MyDrive/model_lstm_attn_26_april.pt')\n",
        "    print('Epoch {} training is finished, loss: {:.4f}'.format(epoch+1, total_loss/total_batches))\n",
        "\n",
        "    desc = 'Validating epoch {}'.format(epoch+1)\n",
        "    scores = score(model, x_development.to(device), dev_target, desc=desc)\n",
        "    scores_str = '\\n'.join(['{}: {:.4f}'.format(name, score) for name, score in scores.items()])\n",
        "    scores_history.append(scores)\n",
        "\n",
        "    print ('Epoch {} validation is finished.\\n{}'.format(\n",
        "        epoch+1, scores_str\n",
        "    ))\n",
        "\n",
        "    metric = scores[scoring_metric]\n",
        "    \n",
        "    # Early Stop\n",
        "    if metric > best_score:\n",
        "        early_stop_counter = 0\n",
        "        print('The best model is found, resetting early stop counter.')\n",
        "        best_score = metric\n",
        "        best_model = model\n",
        "        torch.save(best_model.state_dict(), '/content/drive/MyDrive/model_best_lstm_attn_26_April.pt')\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        print('No improvements for {} epochs.'.format(early_stop_counter))\n",
        "        if early_stop_counter >= early_stop_after:\n",
        "            print('Early stop!')\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to2GLKqgWl54"
      },
      "source": [
        "**Evaluation on validation set separated from training set provided**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9sqqx7zneq"
      },
      "source": [
        "#checking the model's performance on the dev set prepared from the training dataset provided\n",
        "examples = zip(dev_source[10:20], dev_target[10:20], x_development[10:20]) #checked for only 10 sentences\n",
        "for source, target, x in examples:\n",
        "    y = model(x.unsqueeze(0))\n",
        "    translation = ' '.join(unindex_words(output_lang,y[1:-1]))\n",
        "    source = ' '.join(source)\n",
        "    references = '\\n'.join([' '.join(t) for t in target])\n",
        "\n",
        "    print('Source: \"{}\"\\nReferences:\\n{}\\nTranslation: \"{}\"\\n'.format(source, references, translation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6dPTlCTGufB"
      },
      "source": [
        "**Performing Translation on the Test Set Provided**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUoj17jzGzkk"
      },
      "source": [
        "#loading the test set provided for final week\n",
        "column_names = []\n",
        "test_final = []\n",
        "  \n",
        "# reading csv file\n",
        "with open('/content/drive/MyDrive/testhindistatements.csv', 'r') as csvfile:\n",
        "    # creating a csv reader object\n",
        "    csvreader = csv.reader(csvfile)\n",
        "      \n",
        "    # extracting field names through first row\n",
        "    column_names = next(csvreader)\n",
        "  \n",
        "    # extracting each data row one by one\n",
        "    for row in csvreader:\n",
        "        #print(row[2])\n",
        "        test_final.append([row[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FNJFWOVHFyJ"
      },
      "source": [
        "#preprocessing the sentences and storing it in a list\n",
        "test_final_preprocessed=[]\n",
        "for i in range(len(test_final)):\n",
        "    clean_line=pre_process_hindi_sentence(str(test_final[i]))\n",
        "    test_final_preprocessed.append(clean_line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ATc4KLWHFYe"
      },
      "source": [
        "#checking one of the preprocessed text\n",
        "test_final_preprocessed[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4AXYyVVHUun"
      },
      "source": [
        "**Appending SOS and EOS token to hindi sentences and converting the hindi sentences to tensor objects**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lB-Z1GSHO1L"
      },
      "source": [
        "def indexesFromSentence_test(lang, sentence):\n",
        "    list_index=[]\n",
        "    for word in sentence.split(' '):\n",
        "      try:\n",
        "        list_index.append(lang.word2index[word])\n",
        "      except:\n",
        "        #if a word is not present in the vocabulary made from training set I defined it as unknown token\n",
        "        list_index.append(lang.word2index[UNK_token])\n",
        "    return list_index\n",
        "\n",
        "\n",
        "def tensorFromSentence_test(lang, sentence,type):\n",
        "    indexes = indexesFromSentence_test(lang, sentence)\n",
        "    #print(len(indexes))\n",
        "    indexes.append(EOS_idx)\n",
        "    indexes.insert(0, SOS_idx)\n",
        "    if type=='hindi':\n",
        "      max_seq_length=2+max_length_src\n",
        "    else:\n",
        "      max_seq_length=2+max_length_tar\n",
        "    if len(indexes) < max_seq_length:\n",
        "        indexes += [PAD_idx] * (max_seq_length - len(indexes))\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo4TE9DuHbSf"
      },
      "source": [
        "#converting the list to a tensor object so that we can send it as input to our model\n",
        "test_final_tensor=[]\n",
        "for source_sent in test_final_preprocessed:\n",
        "    tensor = tensorFromSentence_test(input_lang, source_sent, 'hindi').unsqueeze(1)\n",
        "    test_final_tensor.append(tensor)\n",
        "test_final_tensor = torch.transpose(torch.cat(test_final_tensor, dim=-1), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVvRKm7GHg2G"
      },
      "source": [
        "#checking the shape of the tensor \n",
        "test_final_tensor.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K_KdtptHkq-"
      },
      "source": [
        "#initialising the model\n",
        "model = Seq2seq(input_lang.n_words, output_lang.n_words, 256, 1,device).to(device)\n",
        "#loading the saved state of the model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/NLP_Challenge_Test_Phase/model_best_lstm_attn_26_April.pt'))\n",
        "examples = zip(test_final_preprocessed,test_final_tensor)\n",
        "file1 = open(\"/content/drive/MyDrive/NLP_Challenge_Test_Phase/answer.txt\",\"w\") #writing the predicted outputs to a file \n",
        "for source,  x in examples:\n",
        "    y = model(x.unsqueeze(0).to(device))\n",
        "    translation = ' '.join(unindex_words(output_lang,y[1:-1]))\n",
        "    source = ' '.join(source)\n",
        "    \n",
        "    file1.write(translation+'\\n')\n",
        "    print('Source: \"{}\"\\nTranslation: \"{}\"\\n'.format(source,  translation)) # I printed the source and model's predicted translations here\n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}